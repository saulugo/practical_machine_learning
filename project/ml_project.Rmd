---
title: "Practical Machine Learning - Course Project"
author: "Saul Lugo"
date: "January 20, 2016"
output: html_document
---

#Abstract
Write here the abstract

#Getting and Cleaning Data
```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, chache=TRUE)
library(ggplot2)
library(Hmisc)
library(caret)
library(dplyr)
```
The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har
```{r getting_data}
url_training <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_testing <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if(!file.exists("./data")) dir.create("./data")
file_training <- "./data/pml-training.csv"
file_testing <- "./data/pml-testing.csv"
if(!file.exists(file_training))
        download.file(url_training,destfile=file_training,method="curl")
if(!file.exists(file_testing))
        download.file(url_testing,destfile=file_testing,method="curl")
training <- read.csv(file_training)
testing <- read.csv(file_testing)
```

##Cleaning the data
From the summary of the training dataset it can be identify the following issues with the data:

1) The **kurtosis** (kurtosis_roll_XXX, kurtosis_picth_XXX, kurtosis_yaw_XXX variables) were render as factor variables. In order to facilitate the analysis, these variables will be converted to continous numeric variables.
2) The **skewness** (skewness_roll_XXX, skewness_picth_XXX, skewness_yaw_XXX variables) are also factors variables and will be converted to numeric variables.
3) The variable **skewness_roll_belt.1** will be dropped. The variable **skewness_roll_belt** will be used in its place.
4) There are a group of variables that have 19216 NAs out of 19622 observations. Those variables will not add valuable information to train the prediction model; therefore, those variables will be eliminated from the analysis.
5) The variables **max_yaw** and **min_yaw** (max_yaw_XXX and min_yaw_XXX variables) were also rendered as factor variables. Those will be converted to numeric continous variables.
6) As max and min roll and picth variables have most of the values as NAs, the **amplitude** variables have also most of the values in NAs, so **amplitude** variables will be discarted from the analysis.
7) The variables max_yaw_XXX and min_yaw_XXX has the same values, as a result the variable amplitude_yaw_XX is always zero. So, these variables do not add information and will be discarted from the analysis.
8) As the machine learning algorithm will be trained to predict if the exercise was done well, the variables related to timestamp, window and the name of the person executing the exercise are irrelevant. Those variables will be dropped.

The following code chunk deals with the cleaning of the data following the previous 8 remarks about the dataset:

```{r cleaning_data}
#eliminating columns with more than 80% of the values in NAs
training <- training[,!colSums(is.na(training))>0.8*nrow(training)]
#transforming to numeric the kurtosis variables
training[,grep("kurtosis",names(training))] <- lapply(training[,grep("kurtosis",names(training))],as.numeric)
#transforming to numeric the skewness variables
training[,grep("skewness",names(training))] <- lapply(training[,grep("skewness",names(training))],as.numeric)
#eliminating the column "skewness_roll_belt.1""
training <- training[,names(training)!="skewness_roll_belt.1"]
#eliminating the variables max_yaw_XXX, min_yaw_XXX, amplitude_yaw_XXX
training <- training[,-grep("max_yaw",names(training))]
training <- training[,-grep("min_yaw",names(training))]
training <- training[,-grep("amplitude_yaw",names(training))]
#eliminating irrelevant variables for the analysis
training <- training[,names(training)!="user_name"]
training <- training[,-grep("timestamp",names(training))]
training <- training[,-grep("window",names(training))]
training <- training[,-1]
dim(training)
```

#Exploratory Data Analysis
The sensors that the individuals in the experiment were wearing provide the following measurements: the euler angles (roll, pitch and yaw) at each step of the sliding window and the raw accelerometer, gyroscope and magnetometer readings. One can expect that the likelihood that a weight lifting exercise is being executed correctly or incorrectly will depend highly in the position of the dumbbell at each time, then the euler angles for the dumbbell should be very important predictors in this experiment.

Following this reasoning, let's explore some plots of the relationshipt of the euler angles in the dumbbell and the outcome of the exercises:

```{r exploratory_1}
library(ggplot2)
library(plot3D)

par(mar=c(5.1,4.1,4.1,2.1))
x <- training$roll_dumbbell
y <- training$pitch_dumbbell
z <- training$yaw_dumbbell
scatter3D(x,y,z,colvar=as.integer(training$classe),col=c("green","red","blue","black","orange"),pch=16,cex=1,clim=c(1,5),ticktype="detailed",phi=20,theta=20,xlab="roll",ylab="pitch",zlab="yaw",main="Exercise Execution vs Dumbbell Euler Angles",colkey = list(at = c(1,2,3,4,5),side = 1,addlines=FALSE,length=0.5,width=0.5,labels=c("A","B","C","D","E")))
```

It can be observed in the previous plot that the yaw angle in the dumbbell differences the correct and incorrect execution of the exercise.

Also, differentiation can be observed if we change the z variable for other measurements like the total accelerometer of the dumbbell, and plot it against the dumbbell's roll and pitch angle:

```{r exploratory_2}
# Let's change z to be the total_accel_dumbbell variable
z <- training$total_accel_dumbbell
scatter3D(x,y,z,colvar=as.integer(training$classe),col=c("green","red","blue","black","orange"),pch=16,cex=1,clim=c(1,5),ticktype="detailed",phi=20,theta=20,xlab="roll",ylab="pitch",zlab="total_accel",main="Dumbbell - roll/pitch vs total_accel",colkey = list(at = c(1,2,3,4,5),side = 1,addlines=FALSE,length=0.5,width=0.5,labels=c("A","B","C","D","E")))
```

As with the last plot, separation can be appreciated changing the variable z by the dumbbell magnet_z:

```{r exploratory_3}
# Let's change z to be the total_accel_dumbbell variable
z <- training$magnet_dumbbell_z
scatter3D(x,y,z,colvar=as.integer(training$classe),col=c("green","red","blue","black","orange"),pch=16,cex=1,clim=c(1,5),ticktype="detailed",phi=20,theta=20,xlab="roll",ylab="pitch",zlab="magnet_z",main="Dumbbell - roll/pitch vs magnet_z",colkey = list(at = c(1,2,3,4,5),side = 1,addlines=FALSE,length=0.5,width=0.5,labels=c("A","B","C","D","E")))
```

It is intuitive to assume that the Euler Angles in the other sensors (arm, forearm and belt) will also predict some separation in the outcome of the exercise. The following plot shows the Euler Angles vs the Outcome in the arm sensor:

```{r exploratory_4}
library(ggplot2)
library(plot3D)

par(mar=c(5.1,4.1,4.1,2.1))
x <- training$roll_arm
y <- training$pitch_arm
z <- training$yaw_arm
scatter3D(x,y,z,colvar=as.integer(training$classe),col=c("green","red","blue","black","orange"),pch=16,cex=1,clim=c(1,5),ticktype="detailed",phi=20,theta=20,xlab="roll",ylab="pitch",zlab="yaw",main="Exercise Execution vs Arm Euler Angles",colkey = list(at = c(1,2,3,4,5),side = 1,addlines=FALSE,length=0.5,width=0.5,labels=c("A","B","C","D","E")))
```

The following plot shows the angles in the Belt Sensor:

```{r exploratory_5}
library(ggplot2)
library(plot3D)

par(mar=c(5.1,4.1,4.1,2.1))
x <- training$roll_belt
y <- training$pitch_belt
z <- training$yaw_belt
scatter3D(x,y,z,colvar=as.integer(training$classe),col=c("green","red","blue","black","orange"),pch=16,cex=1,clim=c(1,5),ticktype="detailed",phi=20,theta=20,xlab="roll",ylab="pitch",zlab="yaw",main="Exercise Execution vs Belt Euler Angles",colkey = list(at = c(1,2,3,4,5),side = 1,addlines=FALSE,length=0.5,width=0.5,labels=c("A","B","C","D","E")))
```

And finally the following plot shows the angles in the forearm sensor:

```{r exploratory_6}
library(ggplot2)
library(plot3D)

par(mar=c(5.1,4.1,4.1,2.1))
x <- training$roll_forearm
y <- training$pitch_forearm
z <- training$yaw_forearm
scatter3D(x,y,z,colvar=as.integer(training$classe),col=c("green","red","blue","black","orange"),pch=16,cex=1,clim=c(1,5),ticktype="detailed",phi=20,theta=20,xlab="roll",ylab="pitch",zlab="yaw",main="Exercise Execution vs Forearm Euler Angles",colkey = list(at = c(1,2,3,4,5),side = 1,addlines=FALSE,length=0.5,width=0.5,labels=c("A","B","C","D","E")))
```

As a conclusion from the Exploratory Data Analysis, it can be appreciated that the Euler Angles show separation in all the sensors, and specially in the Dumbbell sensor. Also, whan an additional variable related to the magnet or the accelerometer is added to the plot separation is apparent as well.

#Predictors Selection

From the Exploratory Data Analysis, it can be inferred that all the sensors measurements related to Euler Angles, gyroscope, magnet and accelerometer will be important as predictors of the outcome of the exercise execution.

However, even after the Data Cleaning, 75 variables still remain in the training dataset.

**The correlation between the variables** was used as the method to reduce the amount of variables (75) to a smaller group that still explains most of the variance in the outcome. As a threshold, the new smaller groupd of variable is conformed for variables that has less than 75% of correlation between all of them. The following code chunk uses the functions **cor()** to build a **correlation matrix** between the variables and **findCorrelation()** from the caret packages to find which variables will exceed this threshold of 75% correlation with the other variables and therefore must be dropped from the analysis:

```{r variables_correlation}
predictors <- training[,-76]
names(predictors)
high_correlation <- findCorrelation(correlation_matrix,cutoff=0.75)
#now, let's eliminate the predictors with correlations higher than .75
predictors <- predictors[,-high_correlation]
names(predictors)
```

Regarding the Euler Angles, the variables correlation algorithms eliminates the roll and pitch angle from the belt sensor and leave the yaw angle. For the dumbbell, arm and forearm leaves all the angles. This in line with the observations done in the Exploratory Data Analysis. Also, the measurements from the gyroscope, accelerometer and magnet that explain the most variance in the outcome were left in the predictors group.

#Training the Model

The model selected for this analysis was **Random Forrest (RF)**. This algorithm is highly accurate for classifications problems. However it is also highly computational demanding, and it can takes hours running in a regular personal computer.

##Parallel Processing Setup
In order to make the RF algorithm more efficient, a parallel setup was used. The computer in which this analysis was run has the following characteristics:
- A macbook air
- 1.3 GHz Inte Core i5 with 4 Cores
- 4 GB of RAM
- OS X Version 10.9.5

The following code chunk configures the parallels processing in 3 of the 4 cores that the computer has:

```{r parallel_processing}
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
```

##Cross-Validation

For cross-validation, a **k-fold** algorithm with k = 10 will be used. In the caret package, the cross-validation is configured in the **trainControl** object:

```{r train_control_object}
fitControl <- trainControl(method="cv",number=10,allowParallel=TRUE)
```

The **allowParallel** parameter tells the caret's train() function to use the parallel processing.

##Model training using parallel processing

